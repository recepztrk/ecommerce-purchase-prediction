# E-Commerce Purchase Prediction
## Proje Sunumu

---

## ğŸ“Œ Proje Ã–zeti

**Hedef:** E-commerce kullanÄ±cÄ± davranÄ±ÅŸlarÄ±ndan alÄ±ÅŸveriÅŸ yapma olasÄ±lÄ±ÄŸÄ±nÄ± tahmin etmek

**BaÅŸlangÄ±Ã§:** v3.0 baseline (Test AUC 0.7619)

**Hedef:** Test AUC 0.78+ (%2.4 iyileÅŸtirme)

**SonuÃ§:** v3.0 hala en iyi model

**Denenen YÃ¶ntemler:** 10 farklÄ± optimizasyon yaklaÅŸÄ±mÄ±

---

## ğŸ¯ v3.0 Baseline PerformansÄ±

| Metrik | DeÄŸer | AÃ§Ä±klama |
|--------|-------|----------|
| Test AUC | 0.7619 | SÄ±ralama yeteneÄŸi |
| Test F1 | 0.69 | Precision-Recall dengesi |
| Precision | 0.65 | DoÄŸruluk |
| **Recall** | **0.98** | **â­ MÃ¼ÅŸteri yakalama** |
| Gap | 11% | DÃ¼ÅŸÃ¼k overfitting |

**GÃ¼Ã§lÃ¼ YÃ¶n:** %98 recall - 100 mÃ¼ÅŸteriden sadece 2'sini kaÃ§Ä±rÄ±yor!

---

## ğŸ”¬ Denenen Optimizasyonlar

### 1. Feature Engineering

**v4.0 - Aggressive Removal (16 features)**
- SonuÃ§: AUC 0.7398 (-2.9%) âŒ
- Neden baÅŸarÄ±sÄ±z: Bilgi kaybÄ±

**v5.0 - Additive (68 features)**
- SonuÃ§: AUC 0.7588 (-0.4%) âŒ
- Neden baÅŸarÄ±sÄ±z: Overfitting (+3% gap)

---

### 2. Algorithm Testing

| Algorithm | AUC | F1 | Recall | SonuÃ§ |
|-----------|-----|-----|--------|-------|
| ExtraTrees | 0.7644 | 0.67 | 0.77 | âŒ Recall dÃ¼ÅŸÃ¼k |
| XGBoost | 0.7623 | 0.68 | 0.84 | âŒ Recall dÃ¼ÅŸÃ¼k |
| LightGBM | 0.7629 | 0.67 | 0.83 | âŒ Recall dÃ¼ÅŸÃ¼k |

**HiÃ§biri v3.0'Ä±n recall'Ä±nÄ± (0.98) yakalayamadÄ±**

---

### 3. Hyperparameter Optimization (Optuna)

**ExtraTrees (25 trials):**
- Test AUC: 0.7751 (+1.73%) âœ…
- F1: 0.67 (-2.9%) âŒ
- Recall: 0.77 (-21%!) âŒ
- Gap: 13.6% (+2.6%) âŒ

**SonuÃ§:** AUC arttÄ± ama dengesi bozuldu

---

### 4. Ensemble Methods (10 YÃ¶ntem!)

**Denenen YÃ¶ntemler:**
1. Grid Search (AUC optimization)
2. Equal Weights  
3. Stacking (Meta-learner)
4. Multi-objective: AUC only
5. Multi-objective: F1 only
6. Multi-objective: AUC + F1
7. Multi-objective: AUC + F1 + Precision
8. Multi-objective: AUC + F1 + Recall
9. Multi-objective: Composite
10. v3.0 Hyperparameter Tuning

**SonuÃ§:** HEPSÄ° baÅŸarÄ±sÄ±z! âŒ

---

### 5. En Ä°yi Ensemble Sonucu

**AUC+F1+Precision Optimization:**
- Test AUC: 0.7702 (+1.09%)
- F1: 0.66 (-4.3%)
- Recall: 0.72 (-26%!) âŒ

**v3.0 hala daha dengeli**

---

## ğŸ“Š KarÅŸÄ±laÅŸtÄ±rma Tablosu

| Model | AUC | F1 | Recall | Gap |
|-------|-----|-----|--------|-----|
| **v3.0** | 0.7619 | **0.69** | **0.98** | **11%** |
| ExtraTrees | **0.7751** | 0.67 | 0.77 | 13.6% |
| Ensemble | 0.7702 | 0.66 | 0.72 | 13.6% |
| v3.0 Tuned | 0.7555 | 0.68 | 0.85 | 13% |

**v3.0: 5 metrikten 3'Ã¼nde #1**

---

## ğŸ“ Ã–ÄŸrenilen Dersler

### 1. Data Quality > Everything
v3.0'Ä±n baÅŸarÄ±sÄ± = Temiz veri (session merging, quality filtering)

### 2. Validation â‰  Test
Validation'da harika â†’ Test'te kÃ¶tÃ¼ (overfitting!)

### 3. Recall Ä°Ã§in HiÃ§bir Åey Feda Edilmez
v3.0'Ä±n recall'Ä± (0.98) = altÄ±n deÄŸerinde

### 4. Ensemble Magic Yoktur
10 yÃ¶ntem denendi, hiÃ§biri iÅŸe yaramadÄ±

### 5. Simple is Beautiful
24 feature + default LightGBM > 68 feature + stacking

---

## ğŸš« "Neden X'i Denemediler?"

âœ… **Feature Engineering** - Denendi, baÅŸarÄ±sÄ±z

âœ… **FarklÄ± Algoritmalar** - 5 algoritma test edildi

âœ… **Ensemble** - 10 yÃ¶ntem denendi

âœ… **Hyperparameter Tuning** - Optuna ile yapÄ±ldÄ±

âŒ **Daha Fazla Veri** - Ä°mkan yok (en etkili olurdu)

âŒ **Deep Learning** - Veri yetersiz, overkill

âŒ **GNN** - Infrastructure yok

---

## âš ï¸ Critical Findings

### ExtraTrees Paradoksu
- Validation AUC: 0.8106 (orta)
- **Test AUC: 0.7751 (en iyi!)**
- Grid search onu hiÃ§ seÃ§medi!
- Validation-test mismatch

### Recall Sorunu
HiÃ§bir optimizasyon v3.0'Ä±n recall'Ä±nÄ± yakalayamadÄ±:
- v3.0: 0.98 (100'de 2 kayÄ±p)
- En iyi diÄŸer: 0.85 (100'de 15 kayÄ±p)
- **13 mÃ¼ÅŸteri farkÄ± = ciddi gelir kaybÄ±**

---

## ğŸ’¡ Ä°ÅŸ DeÄŸeri Perspektifi

### v3.0 KullanÄ±mÄ±

**OlasÄ±lÄ±k SkorlarÄ±:**
```
MÃ¼ÅŸteri A: 0.95 â†’ Kesin kampanya gÃ¶nder
MÃ¼ÅŸteri B: 0.75 â†’ Orta Ã¶ncelik
MÃ¼ÅŸteri C: 0.55 â†’ Ä°ndirim gÃ¶ster
MÃ¼ÅŸteri D: 0.25 â†’ HiÃ§ uÄŸraÅŸma
```

**Avantaj:**
- MÃ¼ÅŸterileri sÄ±ralayabilme
- Budget optimizasyonu
- Dinamik strateji

---

## âœ… Final Karar

### v3.0 Baseline KullanÄ±lacak

**Neden:**
1. En yÃ¼ksek F1 (0.69)
2. En yÃ¼ksek Recall (0.98) â­
3. En yÃ¼ksek Precision (0.65)
4. En dÃ¼ÅŸÃ¼k Gap (11%)
5. Basit ve maintainable

**Ä°ÅŸ DeÄŸeri:**
- 100 mÃ¼ÅŸteriden 98'ini yakalÄ±yor
- Minimal false negative
- ROI maksimum

---

## ğŸ”® Gelecek Ã–neriler

### EÄŸer Kaynak Bulunursa:

**1. Daha Fazla Veri (+2-3% AUC)**
- 10M+ session hedef
- En etkili iyileÅŸtirme

**2. External Features (+1-2% AUC)**
- ÃœrÃ¼n kategorisi detaylarÄ±
- Fiyat trendleri
- Mevsimsellik

**3. A/B Testing**
- GerÃ§ek kullanÄ±cÄ±larla test
- Business metric (ROI) track

---

## ğŸ“ˆ Proje Zaman Ã‡izelgesi

- **Hafta 1-2:** v3.0 analiz, feature engineering
- **Hafta 3:** Algorithm testing
- **Hafta 4-5:** Hyperparameter optimization (Optuna)
- **Hafta 6-7:** 10 ensemble yÃ¶ntemi
- **Hafta 8:** Final analiz ve karar

**Toplam:** ~20 saat computation

---

## ğŸ¯ SonuÃ§

**10 farklÄ± optimizasyon denendi.**

**HiÃ§biri v3.0'dan iyi deÄŸil.**

**Bu baÅŸarÄ±sÄ±zlÄ±k DEÄÄ°L, sistematik optimizasyon!**

Her deneme bize bir ÅŸey Ã¶ÄŸretti:
- Veri kalitesi #1 faktÃ¶r
- Basit modeller gÃ¼Ã§lÃ¼ olabilir
- Tek metrik optimize etmek riskli
- Validation-test gap kritik

---

## ğŸ“Š Teknik Detaylar

**Veri:**
- Train: 2.2M sessions
- Val: 469K sessions
- Test: 541K sessions
- Features: 24

**AraÃ§lar:**
- Python 3.14
- LightGBM, XGBoost, Scikit-learn
- Optuna
- Google Colab (parallel execution)

---

## ğŸ“ Kaynaklar

**Raporlar:**
- `FINAL_PROJECT_REPORT.md` - KapsamlÄ± rapor
- `reports/final_report_v3.md` - v3.0 detaylarÄ±
- `reports/phase3_detailed_metrics.csv` - TÃ¼m metrikler
- `reports/phase4c_multiobjective_results.csv` - Ensemble sonuÃ§larÄ±

**Modeller:**
- `models/` klasÃ¶rÃ¼nde tÃ¼m modeller
- v3.0 GitHub'da mevcut

---

## ğŸ™ TeÅŸekkÃ¼rler

**Sorular?**

---

## ğŸ“ Ek: Metrik AÃ§Ä±klamalarÄ±

**AUC (Area Under ROC Curve):**
- Model'in sÄ±ralama yeteneÄŸi
- 0.5 = Rastgele, 1.0 = MÃ¼kemmel
- v3.0: 0.7619 = Ä°yi

**F1 Score:**
- Precision ve Recall'Ä±n harmonik ortalamasÄ±
- Dengeli performans gÃ¶stergesi
- v3.0: 0.69 = Ã‡ok iyi

**Recall:**
- TÃ¼m pozitif Ã¶rneklerin yakalanma oranÄ±
- v3.0: 0.98 = Neredeyse mÃ¼kemmel!
- Ä°ÅŸ deÄŸeri aÃ§Ä±sÄ±ndan en kritik

**Precision:**
- Pozitif tahminlerin doÄŸruluÄŸu
- v3.0: 0.65 = Ä°yi

**Gap:**
- Train-Test AUC farkÄ±
- Overfitting gÃ¶stergesi
- v3.0: 11% = DÃ¼ÅŸÃ¼k (iyi!)
